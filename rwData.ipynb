{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and writing data -- _Tour of Beam_\n",
    "\n",
    "So far we've learned some of the basic transforms like\n",
    "[`Map`](https://beam.apache.org/documentation/transforms/python/elementwise/map),\n",
    "[`FlatMap`](https://beam.apache.org/documentation/transforms/python/elementwise/flatmap),\n",
    "[`Filter`](https://beam.apache.org/documentation/transforms/python/elementwise/filter),\n",
    "[`Combine`](https://beam.apache.org/documentation/transforms/python/aggregation/combineglobally), and\n",
    "[`GroupByKey`](https://beam.apache.org/documentation/transforms/python/aggregation/groupbykey).\n",
    "These allow us to transform data in any way, but so far we've used\n",
    "[`Create`](https://beam.apache.org/documentation/transforms/python/other/create)\n",
    "to get data from an in-memory\n",
    "[`iterable`](https://docs.python.org/3/glossary.html#term-iterable), like a `list`.\n",
    "\n",
    "This works well for experimenting with small datasets. For larger datasets we can use `Source` transforms to read data and `Sink` transforms to write data.\n",
    "If there are no built-in `Source` or `Sink` transforms, we can also easily create our custom I/O transforms.\n",
    "\n",
    "Let's create some data files and see how we can read them in Beam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/my-text-file-1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/my-text-file-1.txt\n",
    "This is just a plain text file, UTF-8 strings are allowed ðŸŽ‰.\n",
    "Each line in the file is one element in the PCollection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/my-text-file-2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/my-text-file-2.txt\n",
    "There are no guarantees on the order of the elements.\n",
    "à¸…^â€¢ï»Œâ€¢^à¸…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/penguins.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/penguins.csv\n",
    "species,culmen_length_mm,culmen_depth_mm,flipper_length_mm,body_mass_g\n",
    "0,0.2545454545454545,0.6666666666666666,0.15254237288135594,0.2916666666666667\n",
    "0,0.26909090909090905,0.5119047619047618,0.23728813559322035,0.3055555555555556\n",
    "1,0.5236363636363636,0.5714285714285713,0.3389830508474576,0.2222222222222222\n",
    "1,0.6509090909090909,0.7619047619047619,0.4067796610169492,0.3333333333333333\n",
    "2,0.509090909090909,0.011904761904761862,0.6610169491525424,0.5\n",
    "2,0.6509090909090909,0.38095238095238104,0.9830508474576272,0.8333333333333334"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from text files\n",
    "\n",
    "We can use the\n",
    "[`ReadFromText`](https://beam.apache.org/releases/pydoc/current/apache_beam.io.textio.html#apache_beam.io.textio.ReadFromText)\n",
    "transform to read text files into `str` elements.\n",
    "\n",
    "It takes a\n",
    "[_glob pattern_](https://en.wikipedia.org/wiki/Glob_%28programming%29)\n",
    "as an input, and reads all the files that match that pattern.\n",
    "It returns one element for each line in the file.\n",
    "\n",
    "For example, in the pattern `data/*.txt`, the `*` is a wildcard that matches anything. This pattern matches all the files in the `data/` directory with a `.txt` extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no guarantees on the order of the elements.\n",
      "à¸…^â€¢ï»Œâ€¢^à¸…\n",
      "This is just a plain text file, UTF-8 strings are allowed ðŸŽ‰.\n",
      "Each line in the file is one element in the PCollection.\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "files = 'data/*.txt'\n",
    "\n",
    "with beam.Pipeline ()as pipe:\n",
    "    result = (\n",
    "        pipe\n",
    "        | \"Read files\" >> beam.io.ReadFromText(files)\n",
    "        | \"Print contents\" >> beam.Map(print)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to text files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_prefix = 'output/file'\n",
    "\n",
    "text = [\n",
    "          'Each element must be a string.',\n",
    "          'It writes one element per line.',\n",
    "          'There are no guarantees on the line order.',\n",
    "          'The data might be written into multiple files.',\n",
    "      ]\n",
    "\n",
    "with beam.Pipeline() as pipe:\n",
    "    (\n",
    "        pipe\n",
    "        | \"Create file lines\" >> beam.Create(text)\n",
    "        | \"Write to files\" >> beam.io.WriteToText(\n",
    "            output_file_prefix,\n",
    "            file_name_suffix=\".txt\"\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each element must be a string.\n",
      "It writes one element per line.\n",
      "There are no guarantees on the line order.\n",
      "The data might be written into multiple files.\n"
     ]
    }
   ],
   "source": [
    "# Previewing the output\n",
    "!head output/file*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from an `iterable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from typing import Iterable\n",
    "\n",
    "def counts(n: int) -> Iterable[int]:\n",
    "    for i in range(n):\n",
    "        yield i\n",
    "\n",
    "# counts(4)\n",
    "n = 5\n",
    "with beam.Pipeline() as pipeline:\n",
    "    (\n",
    "        pipeline \n",
    "        | \"Create inputs\" >> beam.Create([n,6])\n",
    "        | \"Generate elements\" >> beam.FlatMap(counts)\n",
    "        | \"Print elements\" >> beam.Map(print)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an input transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from typing import Iterable\n",
    "\n",
    "@beam.ptransform_fn\n",
    "@beam.typehints.with_input_types(beam.pvalue.PBegin)\n",
    "@beam.typehints.with_output_types(int)\n",
    "def Count(pbegin: beam.pvalue.PBegin, n: int) -> beam.PCollection[int]:\n",
    "    def count(n: int) -> Iterable[int]:\n",
    "        for i in range(n):\n",
    "            yield i\n",
    "\n",
    "    return (\n",
    "        pbegin\n",
    "        | \"Create inputs\" >> beam.Create([n])\n",
    "        | \"Generate element\" >> beam.FlatMap(count)\n",
    "    )\n",
    "\n",
    "\n",
    "n = 5\n",
    "options = PipelineOptions(flags=[], type_check_additional='all')\n",
    "with beam.Pipeline(options=options) as pipeline:\n",
    "    (\n",
    "        pipeline\n",
    "        | f\"Count to {n}\" >> Count(n)\n",
    "        | \"Print elements\" >> beam.Map(print)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Reading CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'species': '0', 'culmen_length_mm': '0.2545454545454545', 'culmen_depth_mm': '0.6666666666666666', 'flipper_length_mm': '0.15254237288135594', 'body_mass_g': '0.2916666666666667'}\n",
      "{'species': '0', 'culmen_length_mm': '0.26909090909090905', 'culmen_depth_mm': '0.5119047619047618', 'flipper_length_mm': '0.23728813559322035', 'body_mass_g': '0.3055555555555556'}\n",
      "{'species': '1', 'culmen_length_mm': '0.5236363636363636', 'culmen_depth_mm': '0.5714285714285713', 'flipper_length_mm': '0.3389830508474576', 'body_mass_g': '0.2222222222222222'}\n",
      "{'species': '1', 'culmen_length_mm': '0.6509090909090909', 'culmen_depth_mm': '0.7619047619047619', 'flipper_length_mm': '0.4067796610169492', 'body_mass_g': '0.3333333333333333'}\n",
      "{'species': '2', 'culmen_length_mm': '0.509090909090909', 'culmen_depth_mm': '0.011904761904761862', 'flipper_length_mm': '0.6610169491525424', 'body_mass_g': '0.5'}\n",
      "{'species': '2', 'culmen_length_mm': '0.6509090909090909', 'culmen_depth_mm': '0.38095238095238104', 'flipper_length_mm': '0.9830508474576272', 'body_mass_g': '0.8333333333333334'}\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io.filesystems import FileSystems as beam_fs\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import codecs\n",
    "import csv\n",
    "from typing import Dict, Iterable, List\n",
    "\n",
    "@beam.ptransform_fn\n",
    "@beam.typehints.with_input_types(beam.pvalue.PBegin)\n",
    "@beam.typehints.with_output_types(Dict[str, str])\n",
    "def ReadCsvFiles(pbegin: beam.pvalue.PBegin, file_patterns: List[str]) -> beam.PCollection[Dict[str, str]]:\n",
    "    def expand_pattern(pattern: str) -> Iterable[str]:\n",
    "        for match_result in beam_fs.match([pattern])[0].metadata_list:\n",
    "            yield match_result.path\n",
    "\n",
    "    def read_csv_lines(file_name: str) -> Iterable[Dict[str, str]]:\n",
    "        with beam_fs.open(file_name) as f:\n",
    "            # Beam reads files as bytes, but csv expects strings,\n",
    "            # so we need to decode the bytes into utf-8 strings.\n",
    "            for row in csv.DictReader(codecs.iterdecode(f, 'utf-8')):\n",
    "                yield dict(row)\n",
    "\n",
    "    return (\n",
    "        pbegin\n",
    "        | \"Create file patterns\" >> beam.Create(file_patterns)\n",
    "        | \"Expand file patterns\" >> beam.FlatMap(expand_pattern)\n",
    "        | \"Read CSV lines\" >> beam.FlatMap(read_csv_lines)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "input_patterns = [\"data/*.csv\"]\n",
    "options = PipelineOptions(flags=[], type_check_additional='all')\n",
    "with beam.Pipeline(options=options) as pipeline:\n",
    "    (\n",
    "        pipeline \n",
    "        | \"Read CSV files\" >> ReadCsvFiles(input_patterns)\n",
    "        | \"Print elements\" >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `DoFn`(a function object used with the `ParDo` transform), \n",
    "- `CombineFn` (a function object used with the `Combine` transform), and\n",
    "- `WindowFn` (a function object used with the `Window` transform).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Reading from a SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'ðŸŒ•', '2017-12-03 15:47:00', 'Full Moon')\n",
      "(2, 'ðŸŒ—', '2017-12-10 07:51:00', 'Last Quarter')\n",
      "(3, 'ðŸŒ‘', '2017-12-18 06:30:00', 'New Moon')\n",
      "(4, 'ðŸŒ“', '2017-12-26 09:20:00', 'First Quarter')\n",
      "(5, 'ðŸŒ•', '2018-01-02 02:24:00', 'Full Moon')\n",
      "(6, 'ðŸŒ—', '2018-01-08 22:25:00', 'Last Quarter')\n",
      "(7, 'ðŸŒ‘', '2018-01-17 02:17:00', 'New Moon')\n",
      "(8, 'ðŸŒ“', '2018-01-24 22:20:00', 'First Quarter')\n",
      "(9, 'ðŸŒ•', '2018-01-31 13:27:00', 'Full Moon')\n"
     ]
    }
   ],
   "source": [
    "#@title Creating the SQLite database\n",
    "import sqlite3\n",
    "\n",
    "database_file = \"moon-phases.db\" #@param {type:\"string\"}\n",
    "\n",
    "with sqlite3.connect(database_file) as db:\n",
    "  cursor = db.cursor()\n",
    "\n",
    "  # Create the moon_phases table.\n",
    "  cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS moon_phases (\n",
    "      id INTEGER PRIMARY KEY,\n",
    "      phase_emoji TEXT NOT NULL,\n",
    "      peak_datetime DATETIME NOT NULL,\n",
    "      phase TEXT NOT NULL)''')\n",
    "\n",
    "  # Truncate the table if it's already populated.\n",
    "  cursor.execute('DELETE FROM moon_phases')\n",
    "\n",
    "  # Insert some sample data.\n",
    "  insert_moon_phase = 'INSERT INTO moon_phases(phase_emoji, peak_datetime, phase) VALUES(?, ?, ?)'\n",
    "  cursor.execute(insert_moon_phase, ('ðŸŒ•', '2017-12-03 15:47:00', 'Full Moon'))\n",
    "  cursor.execute(insert_moon_phase, ('ðŸŒ—', '2017-12-10 07:51:00', 'Last Quarter'))\n",
    "  cursor.execute(insert_moon_phase, ('ðŸŒ‘', '2017-12-18 06:30:00', 'New Moon'))\n",
    "  cursor.execute(insert_moon_phase, ('ðŸŒ“', '2017-12-26 09:20:00', 'First Quarter'))\n",
    "  cursor.execute(insert_moon_phase, ('ðŸŒ•', '2018-01-02 02:24:00', 'Full Moon'))\n",
    "  cursor.execute(insert_moon_phase, ('ðŸŒ—', '2018-01-08 22:25:00', 'Last Quarter'))\n",
    "  cursor.execute(insert_moon_phase, ('ðŸŒ‘', '2018-01-17 02:17:00', 'New Moon'))\n",
    "  cursor.execute(insert_moon_phase, ('ðŸŒ“', '2018-01-24 22:20:00', 'First Quarter'))\n",
    "  cursor.execute(insert_moon_phase, ('ðŸŒ•', '2018-01-31 13:27:00', 'Full Moon'))\n",
    "\n",
    "  # Query for the data in the table to make sure it's populated.\n",
    "  cursor.execute('SELECT * FROM moon_phases')\n",
    "  for row in cursor.fetchall():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'phase_emoji': 'ðŸŒ•', 'peak_datetime': '2017-12-03 15:47:00', 'phase': 'Full Moon'}\n",
      "{'phase_emoji': 'ðŸŒ—', 'peak_datetime': '2017-12-10 07:51:00', 'phase': 'Last Quarter'}\n",
      "{'phase_emoji': 'ðŸŒ‘', 'peak_datetime': '2017-12-18 06:30:00', 'phase': 'New Moon'}\n",
      "{'phase_emoji': 'ðŸŒ“', 'peak_datetime': '2017-12-26 09:20:00', 'phase': 'First Quarter'}\n",
      "{'phase_emoji': 'ðŸŒ•', 'peak_datetime': '2018-01-02 02:24:00', 'phase': 'Full Moon'}\n",
      "{'phase_emoji': 'ðŸŒ—', 'peak_datetime': '2018-01-08 22:25:00', 'phase': 'Last Quarter'}\n",
      "{'phase_emoji': 'ðŸŒ‘', 'peak_datetime': '2018-01-17 02:17:00', 'phase': 'New Moon'}\n",
      "{'phase_emoji': 'ðŸŒ“', 'peak_datetime': '2018-01-24 22:20:00', 'phase': 'First Quarter'}\n",
      "{'phase_emoji': 'ðŸŒ•', 'peak_datetime': '2018-01-31 13:27:00', 'phase': 'Full Moon'}\n",
      "{'phase_emoji': 'ðŸŒ•', 'phase': 'Full Moon'}\n",
      "{'phase_emoji': 'ðŸŒ—', 'phase': 'Last Quarter'}\n",
      "{'phase_emoji': 'ðŸŒ‘', 'phase': 'New Moon'}\n",
      "{'phase_emoji': 'ðŸŒ“', 'phase': 'First Quarter'}\n",
      "{'phase_emoji': 'ðŸŒ•', 'phase': 'Full Moon'}\n",
      "{'phase_emoji': 'ðŸŒ—', 'phase': 'Last Quarter'}\n",
      "{'phase_emoji': 'ðŸŒ‘', 'phase': 'New Moon'}\n",
      "{'phase_emoji': 'ðŸŒ“', 'phase': 'First Quarter'}\n",
      "{'phase_emoji': 'ðŸŒ•', 'phase': 'Full Moon'}\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import sqlite3 as sq\n",
    "from typing import Iterable, List, Tuple\n",
    "\n",
    "class SQLiteSelect(beam.DoFn):\n",
    "    def __init__(self, database_file: str):\n",
    "        self.database_file = database_file\n",
    "        self.connection = None\n",
    "\n",
    "    def setup(self):\n",
    "        self.connection = sq.connect(self.database_file)\n",
    "\n",
    "    def process(self, query: Tuple[str, List[str]]) -> Iterable[Dict[str, str]]:\n",
    "        table, columns = query\n",
    "        cursor = self.connection.cursor()\n",
    "        cursor.execute(f\"SELECT {','.join(columns)} FROM {table}\")\n",
    "        for row in cursor.fetchall():\n",
    "            yield dict(zip(columns, row))\n",
    "\n",
    "    def teardown(self):\n",
    "        self.connection.close()\n",
    "\n",
    "\n",
    "@beam.ptransform_fn\n",
    "@beam.typehints.with_input_types(beam.pvalue.PBegin)\n",
    "@beam.typehints.with_output_types(Dict[str,str])\n",
    "def SelectFromoSQLite(\n",
    "    pbegin: beam.pvalue.PBegin,\n",
    "    database_file: str,\n",
    "    queries: List[Tuple[str, List[str]]],\n",
    ") -> beam.PCollection[Dict[str, str]]:\n",
    "    return (\n",
    "        pbegin\n",
    "        | \"Create None\" >> beam.Create(queries)\n",
    "        | \"SQLite SELECT\" >> beam.ParDo(SQLiteSelect(database_file))\n",
    "    )\n",
    "\n",
    "\n",
    "queries = [\n",
    "    # (table_name, [column1, column2, ...])\n",
    "    ('moon_phases', ['phase_emoji', 'peak_datetime', 'phase']),\n",
    "    ('moon_phases', ['phase_emoji', 'phase']),\n",
    "]\n",
    "\n",
    "options = PipelineOptions(flags=[], type_check_additional='all')\n",
    "with beam.Pipeline(options=options) as pipeline:\n",
    "    (\n",
    "        pipeline\n",
    "        | \"Read from SQLite\" >> SelectFromoSQLite(database_file, queries)\n",
    "        | \"Print rows\" >> beam.Map(print)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
